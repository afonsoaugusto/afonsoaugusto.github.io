<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Domain2 - Self-Attention | Afonso Rodrigues - DevOps & SRE Blog</title><meta name=keywords content><meta name=description content="O mecanismo de Self-Attention (autoatenção) é uma inovação crucial da arquitetura Transformer que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.
Funcionamento Detalhado:
Para cada token na sequência de entrada, a camada de autoatenção calcula três vetores:"><meta name=author content="Afonso Rodrigues"><link rel=canonical href=https://afonsorodrigues.com/posts/self-attention/><meta name=google-site-verification content="G-8TFSN8203P"><link crossorigin=anonymous href=/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn+yY=" rel="preload stylesheet" as=style><link rel=icon href=https://afonsorodrigues.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://afonsorodrigues.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://afonsorodrigues.com/favicon-32x32.png><link rel=apple-touch-icon href=https://afonsorodrigues.com/apple-touch-icon.png><link rel=mask-icon href=https://afonsorodrigues.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://afonsorodrigues.com/posts/self-attention/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://afonsorodrigues.com/posts/self-attention/"><meta property="og:site_name" content="Afonso Rodrigues - DevOps & SRE Blog"><meta property="og:title" content="Domain2 - Self-Attention"><meta property="og:description" content="O mecanismo de Self-Attention (autoatenção) é uma inovação crucial da arquitetura Transformer que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.
Funcionamento Detalhado:
Para cada token na sequência de entrada, a camada de autoatenção calcula três vetores:"><meta property="og:locale" content="pt-br"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-07T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-07T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Domain2 - Self-Attention"><meta name=twitter:description content="O mecanismo de Self-Attention (autoatenção) é uma inovação crucial da arquitetura Transformer que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.
Funcionamento Detalhado:
Para cada token na sequência de entrada, a camada de autoatenção calcula três vetores:"><meta name=twitter:site content="@Afonsoavr"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://afonsorodrigues.com/posts/"},{"@type":"ListItem","position":2,"name":"Domain2 - Self-Attention","item":"https://afonsorodrigues.com/posts/self-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Domain2 - Self-Attention","name":"Domain2 - Self-Attention","description":"O mecanismo de Self-Attention (autoatenção) é uma inovação crucial da arquitetura Transformer que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.\nFuncionamento Detalhado:\nPara cada token na sequência de entrada, a camada de autoatenção calcula três vetores:\n","keywords":[],"articleBody":"O mecanismo de Self-Attention (autoatenção) é uma inovação crucial da arquitetura Transformer que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.\nFuncionamento Detalhado:\nPara cada token na sequência de entrada, a camada de autoatenção calcula três vetores:\nVetor de Consulta (Query - Q): Representa o que o token “está procurando” ou com o que ele está relacionado dentro da sequência. Vetor de Chave (Key - K): Representa o que outros tokens “oferecem” ou sobre o que eles contêm informações relevantes. Vetor de Valor (Value - V): Contém a informação real associada a cada token, que será agregada para formar a saída. Esses vetores são obtidos através da multiplicação do embedding do token de entrada por três matrizes de pesos diferentes, aprendidas durante o treinamento do modelo.\nO processo de autoatenção para cada token de entrada envolve os seguintes passos:\nCálculo das Pontuações de Atenção (Attention Scores): Para cada par de tokens na sequência, o modelo calcula uma pontuação que indica o quão relevante um token é para outro. Isso é tipicamente feito calculando o produto escalar entre o vetor de consulta de um token e o vetor de chave de outro token. Produtos escalares maiores indicam maior similaridade e, portanto, maior relevância.\nEscalonamento: As pontuações são então divididas pela raiz quadrada da dimensão dos vetores de chave (dk). Isso ajuda a estabilizar os gradientes durante o treinamento, especialmente para dimensões de vetores maiores.\nAplicação da Função Softmax: As pontuações escalonadas são passadas por uma função softmax. O softmax normaliza as pontuações, transformando-as em probabilidades que somam 1 para cada token de consulta. Essas probabilidades representam os “pesos de atenção” – o quanto cada token na sequência de entrada deve contribuir para a representação do token de consulta.\nCálculo da Soma Ponderada: Finalmente, o modelo calcula uma soma ponderada dos vetores de valor de todos os tokens na sequência. Os pesos utilizados nesta soma são as probabilidades de atenção calculadas no passo anterior. Os tokens que receberam maior atenção (maiores pesos) terão uma contribuição maior para o vetor de saída do token de consulta.\nO resultado desse processo é um novo vetor para cada token de entrada, que agora contém informações contextuais ponderadas de todos os outros tokens na sequência, com base em sua relevância para o token original.\nMúltiplas Cabeças de Atenção (Multi-Head Attention):\nOs Transformers frequentemente empregam o conceito de “múltiplas cabeças de atenção”. Em vez de realizar o processo de autoatenção apenas uma vez, a entrada é projetada em múltiplos conjuntos de vetores de consulta, chave e valor (cada conjunto representando uma “cabeça”). O processo de autoatenção é realizado independentemente em cada cabeça, e as saídas resultantes são concatenadas e, em seguida, projetadas novamente em uma única saída. Isso permite que o modelo capture diferentes tipos de relações e informações dentro da sequência, melhorando sua capacidade de compreensão contextual.\nBenefícios da Autoatenção:\nCaptura de Dependências de Longo Alcance: A autoatenção permite que o modelo estabeleça conexões diretas entre tokens distantes na sequência, superando a limitação das RNNs de terem dificuldade em aprender dependências de longo prazo devido ao desaparecimento ou explosão dos gradientes. Paralelização: Ao contrário das RNNs, que processam a entrada sequencialmente, o cálculo da autoatenção para diferentes posições na sequência pode ser feito em paralelo, o que acelera significativamente o treinamento e a inferência. Compreensão Contextual: Ao ponderar a importância de diferentes palavras em relação a outras, a autoatenção permite que o modelo entenda o contexto de cada palavra dentro da frase. Por exemplo, na frase “O banco da praça estava sujo, então fui ao banco sacar dinheiro”, a autoatenção ajudaria o modelo a entender que “banco” se refere a lugares diferentes em cada instância. Interpretabilidade (em certa medida): Os pesos de atenção podem fornecer insights sobre quais partes da entrada o modelo considera mais importantes ao processar uma determinada parte. Em resumo, a autoatenção é um mecanismo poderoso que permite aos modelos Transformer processar sequências de forma eficiente e capturar relações complexas entre os elementos, sendo fundamental para o sucesso da IA Generativa em diversas tarefas de linguagem natural e outras modalidades.\n","wordCount":"723","inLanguage":"en","datePublished":"2025-04-07T00:00:00Z","dateModified":"2025-04-07T00:00:00Z","author":{"@type":"Person","name":"Afonso Rodrigues"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://afonsorodrigues.com/posts/self-attention/"},"publisher":{"@type":"Organization","name":"Afonso Rodrigues - DevOps \u0026 SRE Blog","logo":{"@type":"ImageObject","url":"https://afonsorodrigues.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://afonsorodrigues.com/ accesskey=h title="Afonso Rodrigues - DevOps & SRE Blog (Alt + H)">Afonso Rodrigues - DevOps & SRE Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://afonsorodrigues.com/ title=Home><span>Home</span></a></li><li><a href=https://afonsorodrigues.com/about title=Sobre><span>Sobre</span></a></li><li><a href=https://afonsorodrigues.com/utils title=Utils><span>Utils</span></a></li><li><a href=https://afonsorodrigues.com/pages title=Páginas><span>Páginas</span></a></li><li><a href=https://afonsorodrigues.com/archive title=Arquivo><span>Arquivo</span></a></li><li><a href=https://afonsorodrigues.com/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Domain2 - Self-Attention</h1><div class=post-meta><span title='2025-04-07 00:00:00 +0000 UTC'>April 7, 2025</span>&nbsp;·&nbsp;<span>Afonso Rodrigues</span></div></header><div class=post-content><p>O mecanismo de <strong>Self-Attention</strong> (autoatenção) é uma inovação crucial da arquitetura <strong>Transformer</strong> que permite ao modelo ponderar a importância de diferentes partes da sequência de entrada ao processá-la. Diferente de modelos sequenciais anteriores, como as Redes Neurais Recorrentes (RNNs), que processam a entrada um elemento por vez, a autoatenção permite que o modelo estabeleça conexões diretas entre todos os tokens da sequência, independentemente de sua distância posicional.</p><p><strong>Funcionamento Detalhado:</strong></p><p>Para cada token na sequência de entrada, a camada de autoatenção calcula três vetores:</p><ol><li><strong>Vetor de Consulta (Query - Q):</strong> Representa o que o token &ldquo;está procurando&rdquo; ou com o que ele está relacionado dentro da sequência.</li><li><strong>Vetor de Chave (Key - K):</strong> Representa o que outros tokens &ldquo;oferecem&rdquo; ou sobre o que eles contêm informações relevantes.</li><li><strong>Vetor de Valor (Value - V):</strong> Contém a informação real associada a cada token, que será agregada para formar a saída.</li></ol><p>Esses vetores são obtidos através da multiplicação do embedding do token de entrada por três matrizes de pesos diferentes, aprendidas durante o treinamento do modelo.</p><p>O processo de autoatenção para cada token de entrada envolve os seguintes passos:</p><ol><li><p><strong>Cálculo das Pontuações de Atenção (Attention Scores):</strong> Para cada par de tokens na sequência, o modelo calcula uma pontuação que indica o quão relevante um token é para outro. Isso é tipicamente feito calculando o produto escalar entre o vetor de consulta de um token e o vetor de chave de outro token. Produtos escalares maiores indicam maior similaridade e, portanto, maior relevância.</p></li><li><p><strong>Escalonamento:</strong> As pontuações são então divididas pela raiz quadrada da dimensão dos vetores de chave (dk). Isso ajuda a estabilizar os gradientes durante o treinamento, especialmente para dimensões de vetores maiores.</p></li><li><p><strong>Aplicação da Função Softmax:</strong> As pontuações escalonadas são passadas por uma função softmax. O softmax normaliza as pontuações, transformando-as em probabilidades que somam 1 para cada token de consulta. Essas probabilidades representam os &ldquo;pesos de atenção&rdquo; – o quanto cada token na sequência de entrada deve contribuir para a representação do token de consulta.</p></li><li><p><strong>Cálculo da Soma Ponderada:</strong> Finalmente, o modelo calcula uma soma ponderada dos vetores de valor de todos os tokens na sequência. Os pesos utilizados nesta soma são as probabilidades de atenção calculadas no passo anterior. Os tokens que receberam maior atenção (maiores pesos) terão uma contribuição maior para o vetor de saída do token de consulta.</p></li></ol><p>O resultado desse processo é um novo vetor para cada token de entrada, que agora contém informações contextuais ponderadas de todos os outros tokens na sequência, com base em sua relevância para o token original.</p><p><strong>Múltiplas Cabeças de Atenção (Multi-Head Attention):</strong></p><p>Os Transformers frequentemente empregam o conceito de &ldquo;múltiplas cabeças de atenção&rdquo;. Em vez de realizar o processo de autoatenção apenas uma vez, a entrada é projetada em múltiplos conjuntos de vetores de consulta, chave e valor (cada conjunto representando uma &ldquo;cabeça&rdquo;). O processo de autoatenção é realizado independentemente em cada cabeça, e as saídas resultantes são concatenadas e, em seguida, projetadas novamente em uma única saída. Isso permite que o modelo capture diferentes tipos de relações e informações dentro da sequência, melhorando sua capacidade de compreensão contextual.</p><p><strong>Benefícios da Autoatenção:</strong></p><ul><li><strong>Captura de Dependências de Longo Alcance:</strong> A autoatenção permite que o modelo estabeleça conexões diretas entre tokens distantes na sequência, superando a limitação das RNNs de terem dificuldade em aprender dependências de longo prazo devido ao desaparecimento ou explosão dos gradientes.</li><li><strong>Paralelização:</strong> Ao contrário das RNNs, que processam a entrada sequencialmente, o cálculo da autoatenção para diferentes posições na sequência pode ser feito em paralelo, o que acelera significativamente o treinamento e a inferência.</li><li><strong>Compreensão Contextual:</strong> Ao ponderar a importância de diferentes palavras em relação a outras, a autoatenção permite que o modelo entenda o contexto de cada palavra dentro da frase. Por exemplo, na frase &ldquo;O banco da praça estava sujo, então fui ao banco sacar dinheiro&rdquo;, a autoatenção ajudaria o modelo a entender que &ldquo;banco&rdquo; se refere a lugares diferentes em cada instância.</li><li><strong>Interpretabilidade (em certa medida):</strong> Os pesos de atenção podem fornecer insights sobre quais partes da entrada o modelo considera mais importantes ao processar uma determinada parte.</li></ul><p>Em resumo, a autoatenção é um mecanismo poderoso que permite aos modelos Transformer processar sequências de forma eficiente e capturar relações complexas entre os elementos, sendo fundamental para o sucesso da IA Generativa em diversas tarefas de linguagem natural e outras modalidades.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://afonsorodrigues.com/>Afonso Rodrigues - DevOps & SRE Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>